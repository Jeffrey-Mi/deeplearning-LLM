{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import dependencies"
      ],
      "metadata": {
        "id": "PBhMY744haIT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuBIMfOfFnmt"
      },
      "outputs": [],
      "source": [
        "# Install some required libraries\n",
        "# Feel free to add more if you want\n",
        "!pip install -q python-levenshtein torchsummaryX==1.1.0 wandb kaggle pytorch-nlp datasets tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEjQRcahF0fn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "from torch import nn, Tensor\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "# import torchsummary\n",
        "from tqdm.auto import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "import gc\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm as blue_tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import json\n",
        "\n",
        "import math\n",
        "from typing import Optional, List\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "\n",
        "#imports for decoding and distance calculation\n",
        "try:\n",
        "    import wandb\n",
        "    import torchsummaryX\n",
        "    import Levenshtein\n",
        "except:\n",
        "    print(\"Didnt install some/all imports\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create dataset class and generate datasets\n",
        "\n",
        "First generate the .bin files containing the tokenized datasets for the respective task in {task}-prep.py. Then import those datasets as numpy arrays inside `train_text` and `val_text` respectively."
      ],
      "metadata": {
        "id": "PafTTfjNhkod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_text =\n",
        "val_text ="
      ],
      "metadata": {
        "id": "JkwZ7nxIjrMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, text, window): # Feel free to add more arguments\n",
        "        self.text = text\n",
        "        self.window = window\n",
        "        self.length = (len(text) - 1) // (window)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        i = ind * self.window\n",
        "\n",
        "        x = torch.from_numpy((self.text[i:i+self.window]).astype(np.int64))\n",
        "        y = torch.from_numpy((self.text[(i+1):i+1+self.window]).astype(np.int64))\n",
        "\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "u9V8OO8bZn8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ao33nYyVrpiv"
      },
      "outputs": [],
      "source": [
        "train_data = TextDataset(\n",
        "    text=train_text,\n",
        "    window=512,\n",
        ")\n",
        "val_data = TextDataset(\n",
        "    text=val_text,\n",
        "    window=512,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qha_-9Zr7He"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = 32,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = 32,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThBhd4ERr7l1"
      },
      "outputs": [],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    x, y = data\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-dwNq0CDtTi-"
      },
      "source": [
        "# Define model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYBFGtqY2vyX"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'context_len': 512,\n",
        "    'vocab_size': 50304,\n",
        "    'num_heads': 8,\n",
        "    'd_model': 512\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4aY5PanAzsFk"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.c_attn = nn.Linear(n_embd, 3 * n_embd, bias=True)\n",
        "        self.c_proj = nn.Linear(n_embd, n_embd, bias=True)\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "        self.n_head = n_head\n",
        "        self.n_embd = n_embd\n",
        "        self.dropout = 0.1\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        out = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        out = self.drop(self.c_proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        self.sa = CausalSelfAttention(n_embd, n_head)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, max_seq_len= config['context_len'], dropout=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, n_layer=5):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "        self.positional_encoding = PositionalEncoding(config['d_model'])\n",
        "        self.blocks = nn.Sequential(*[DecoderBlock(n_embd=config['d_model'], n_head=config['num_heads']) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config['d_model'])\n",
        "        self.lm_head = nn.Linear(config['d_model'], config['vocab_size'])\n",
        "\n",
        "    def forward(self, toks, targets=None):\n",
        "        B, T = toks.shape\n",
        "        tok_emb = self.token_embedding_table(toks)\n",
        "        x = self.positional_encoding(tok_emb)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def generate(self, tok, max_out):\n",
        "        for _ in range(max_out):\n",
        "            tok_cond = tok[:][-config['context_len']:]\n",
        "            logits = self.forward(tok_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            tok_next = torch.multinomial(probs, num_samples=1)\n",
        "            tok = torch.cat((tok, tok_next), dim=1)\n",
        "        return tok[:, -max_out:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPKoIj0it1jP"
      },
      "outputs": [],
      "source": [
        "model   = Decoder(\n",
        "    n_layer=5\n",
        ").to(DEVICE)\n",
        "\n",
        "\n",
        "print(model)\n",
        "\n",
        "x = torch.randint(low=0, high=100, size=(32, 512))\n",
        "\n",
        "# print(x)\n",
        "\n",
        "torchsummaryX.summary(model, x.to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23DMfXsaU6kj"
      },
      "source": [
        "# Loss Function, Optimizers, Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "216ukmHbU-ol"
      },
      "outputs": [],
      "source": [
        "optimizer   = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=5e-3)\n",
        "\n",
        "criterion   = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "scheduler   = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train function"
      ],
      "metadata": {
        "id": "cHVHQQ5riEiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hFK27xluddT"
      },
      "outputs": [],
      "source": [
        "def calculate_loss(logits, targets, criterion):\n",
        "    B, T, C = logits.shape\n",
        "    logits = logits.view(B*T, C)\n",
        "    targets = targets.view(B*T)\n",
        "\n",
        "    loss    = criterion(logits, targets)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    batch_bar = blue_tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "    window = 4000\n",
        "    window_loss        = 0.0\n",
        "    windex = 0\n",
        "    best_val_loss = float(\"inf\")\n",
        "    epoch = 0\n",
        "\n",
        "    for i, (x, y) in enumerate(dataloader):\n",
        "        windex = i%window\n",
        "\n",
        "        if windex == 0:\n",
        "            print('[TRAIN] \\tEpoch %d \\tLoss: %.4f \\tLr: %.6f'\n",
        "                      % (epoch, (window_loss/window), optimizer.param_groups[0]['lr']))\n",
        "\n",
        "            wandb.log({\n",
        "                'train_loss': (window_loss/window),\n",
        "                'lr'        : optimizer.param_groups[0]['lr']\n",
        "            })\n",
        "\n",
        "            print(\"Saving model\")\n",
        "            torch.save(\n",
        "                {'model_state_dict'         : model.state_dict(),\n",
        "                'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "                'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "                'window_loss'                  : (window_loss/window),\n",
        "                'epoch'                    : epoch},\n",
        "                \"./model.pth\"\n",
        "            )\n",
        "            wandb.save(\"./model.pth\")\n",
        "            print(\"Saved best model\")\n",
        "            print(\"Saving artifact...\")\n",
        "            artifact = wandb.Artifact('model', type='model')\n",
        "            artifact.add_file(\"./model.pth\")\n",
        "            run.log_artifact(artifact)\n",
        "\n",
        "            window_loss = 0.0\n",
        "            epoch += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        global x_g\n",
        "        x_g = x\n",
        "        global y_g\n",
        "        y_g = y\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            raw_predictions = model(x, y)\n",
        "\n",
        "            loss        =  calculate_loss(raw_predictions, y, criterion)\n",
        "\n",
        "            window_loss        += loss.item()\n",
        "\n",
        "        # Backward on the masked loss\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Unscales the gradients of optimizer’s assigned params in-place\n",
        "        scaler.unscale_(optimizer)\n",
        "        # Since the gradients of optimizer’s assigned params are unscaled, clips as usual:\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format((window_loss)/(windex+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhwhevgWQbDX"
      },
      "source": [
        "# Wandb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Xbw_0eAQcoR"
      },
      "outputs": [],
      "source": [
        "# Login to Wandb\n",
        "import wandb\n",
        "wandb.login(key=\"\")\n",
        "# Initialize your Wandb Run Here\n",
        "# Save your model architecture in a txt file, and save the file to Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wb6n_WjW5eaX"
      },
      "outputs": [],
      "source": [
        "run = wandb.init(\n",
        "    name = \"sdf\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id = 'xdpn2pcl', #Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw5\", ### Project should be created in your wandb account\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9iqJNwQ5rvK"
      },
      "outputs": [],
      "source": [
        "### Save your model architecture as a string with str(model)\n",
        "model_arch  = str(model)\n",
        "\n",
        "### Save it in a txt file\n",
        "arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "file_write  = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training loop"
      ],
      "metadata": {
        "id": "P1z-3uKQiN49"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kROhTzyvylW"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "train(model, train_loader, criterion, optimizer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download a checkpoint (optional)\n",
        "Run the below cell to download a checkpoint from WandB, altering the model version as needed."
      ],
      "metadata": {
        "id": "6k_Trri2ujxe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yg_3rReSQGw"
      },
      "outputs": [],
      "source": [
        "# Load checkpoint\n",
        "artifact = run.use_artifact('ojjs/hw5/model:v59', type='model')\n",
        "artifact_dir = artifact.download()\n",
        "artifact_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to load the associated model that was downloaded above, changing the model number."
      ],
      "metadata": {
        "id": "D3XK8RX7uvBu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P72isXQuSuku"
      },
      "outputs": [],
      "source": [
        "checkpoint = torch.load('/content/artifacts/model:v59/model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CgG2L3MXVG9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint  = torch.load(\"./model.pth\")\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "9_pXvG3LWL_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate text\n",
        "\n",
        "Use the code chunk below, indicating a starting prompt and max_token_len to generate text using your model."
      ],
      "metadata": {
        "id": "QR1mSa6kgAB4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g760g4Wmx2dd"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "prompt = \"Context: Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50. Question: What color was used to emphasize the 50th anniversary of the Super Bowl? Answer: \"\n",
        "max_token_len = 128\n",
        "\n",
        "prompt = torch.tensor([enc.encode(text)]).to(DEVICE)\n",
        "model.eval()\n",
        "idx = model.generate(prompt, max_token_len)\n",
        "print(enc.decode(idx[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make predictions for the summarization task\n",
        "Evaluated on the test set of the cnn_dailymail dataset"
      ],
      "metadata": {
        "id": "rIm03bx2iUAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import sacrebleu\n",
        "\n",
        "def preprocess_data(examples):\n",
        "    # Adjust based on your model's max input length\n",
        "    zamps = []\n",
        "    for article in examples['article']:\n",
        "        # add the tags\n",
        "        article = enc.encode_ordinary(article)[:506]\n",
        "        article = enc.encode_ordinary(\"Article: \") + article + enc.encode_ordinary(\" Summary: \")\n",
        "        article = [enc.eot_token]*(512 - len(article)) + article\n",
        "        zamps.append(article)\n",
        "    return zamps\n",
        "\n",
        "def generate_summary(batch, model):\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(batch, 50)\n",
        "    trunc_outs = []\n",
        "    for out in outputs:\n",
        "        out = out.tolist()\n",
        "        if enc.eot_token in out:\n",
        "            out = out[:(out.index(enc.eot_token))]\n",
        "        trunc_outs.append(out)\n",
        "\n",
        "    return enc.decode_batch(trunc_outs)\n",
        "\n",
        "dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
        "data = preprocess_data(dataset['test'])\n",
        "\n",
        "predictions = []\n",
        "for i in tqdm(range(batches)):\n",
        "    batch = data[i:i+32].to(DEVICE)\n",
        "    summ = generate_summary(batch, model)\n",
        "    predictions.extend(summ)\n",
        "\n",
        "bleu_scores = [sacrebleu.raw_corpus_bleu([pred], [[ref]]) for pred, ref in zip(predictions, dataset['test']['highlights'])]\n",
        "average_bleu = sum(score.score for score in bleu_scores) / len(bleu_scores)\n",
        "print(f\"Average BLEU Score: {average_bleu}\")"
      ],
      "metadata": {
        "id": "Rmctedgs_b01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create SQuAD predictions\n",
        "\n",
        "First download `dev-v2.0.json` from the SQuAD website, then run below script. To see scores, run the evaluation script in `eval.py` with the dev set."
      ],
      "metadata": {
        "id": "Ku4kMoMfijZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import blue_tqdm as tqdm\n",
        "import torch\n",
        "\n",
        "def generate_answer(context, question):\n",
        "    input_text = \"Context: \" + context\n",
        "    input_text += \" Question: \" + question\n",
        "    input_text += \" Answer: \"\n",
        "    prompt = torch.tensor([enc.encode(input_text)]).to(DEVICE)\n",
        "\n",
        "    answer_start = len(prompt[0])\n",
        "    model.eval()\n",
        "    model_out = model.generate(prompt, 50)[0].tolist()\n",
        "    try:\n",
        "      answer_end = model_out.index(enc.eot_token)\n",
        "    except:\n",
        "      answer_end = len(model_out)\n",
        "\n",
        "    answer_enc = model_out[answer_start:answer_end]\n",
        "    return enc.decode(answer_enc)\n",
        "\n",
        "# Load the SQuAD development dataset.\n",
        "with open('/content/data/dev-v2.0.json', 'r') as file:\n",
        "    squad_data = json.load(file)\n",
        "\n",
        "answers = {}\n",
        "\n",
        "# Process each article, paragraph, and question.\n",
        "for article in tqdm(squad_data['data']):\n",
        "    for paragraph in article['paragraphs']:\n",
        "        context = paragraph['context']\n",
        "        for qa in paragraph['qas']:\n",
        "            question = qa['question']\n",
        "            id = qa['id']\n",
        "            gen_ans = generate_answer(context, question)\n",
        "            answers[id] = gen_ans\n",
        "\n",
        "for id in tqdm(answers):\n",
        "    new = answers[id].replace('Answer:', '')\n",
        "    answers[id] = new.strip()\n",
        "\n",
        "with open('dev-with-generated-answers.json', 'w') as outfile:\n",
        "    json.dump(answers, outfile, indent=4)"
      ],
      "metadata": {
        "id": "S7FJgqAKm_Ui"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}